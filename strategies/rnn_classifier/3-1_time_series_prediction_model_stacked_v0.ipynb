{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # 0.21.0\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from time import time\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f80aea71b50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom dataset to convert features to inputs of sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, time_steps, transform=None):\n",
    "        self._steps = time_steps\n",
    "        self._transform = transform\n",
    "        self._frames = pd.read_csv(os.path.join(root_dir, csv_file))\n",
    "        \n",
    "        self._y = self._frames.pop(\"y\")\n",
    "        self._X = self._frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._frames.index) - self._steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = idx + self._steps\n",
    "        # Normalize over sequence (TODO: Find a better way to handle)\n",
    "        X = self._X.iloc[index-self._steps:index]\n",
    "        X_mean = X.apply(np.mean,axis=0)\n",
    "        X_std = X.apply(np.std,axis=0)\n",
    "        X_normalized = (X - X_mean)/(X_std+1e-10)\n",
    "        sequence = torch.from_numpy(X_normalized.as_matrix())\n",
    "        \n",
    "        label = torch.from_numpy(np.array([self._y.iloc[index]]))\n",
    "        return {'sequence': sequence, 'label': label}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SequenceDataset('train_data.csv','.',25)\n",
    "test_data = SequenceDataset('test_data.csv','.',25)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, batch_size, steps):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1, self.hidden_size_2, self.hidden_size_3 = hidden_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        \n",
    "        # Try preprocess with FC layer\n",
    "        # https://danijar.com/tips-for-training-recurrent-neural-networks/\n",
    "        self.fc_pre = nn.Linear(self.input_size, self.hidden_size_1)\n",
    "        self.rnn_1 = nn.LSTM(self.hidden_size_1, self.hidden_size_2, num_layers=2, dropout=0.5, batch_first=True)\n",
    "        self.rnn_2 = nn.LSTM(self.hidden_size_2, self.hidden_size_3, num_layers=2, dropout=0.5, batch_first=True)\n",
    "        self.rnn_3 = nn.LSTM(self.hidden_size_3, hidden_size=self.hidden_size_3, num_layers=2, dropout=0.5, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size_3, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "        # Consider learning the hidden initialization\n",
    "        self.h_0 = nn.Parameter(\n",
    "            torch.randn(2, self.batch_size, self.hidden_size).type(torch.FloatTensor),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.c_0 = nn.Parameter(\n",
    "            torch.randn(2, self.batch_size, self.hidden_size).type(torch.FloatTensor),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        out = self.fc_pre(x)\n",
    "        out, _ = self.rnn_1(out)\n",
    "        out, _ = self.rnn_2(out)\n",
    "        out, _ = self.rnn_3(out)\n",
    "        # We want the out of the last step (batch, step, out)\n",
    "        return self.fc(out[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleLSTM(input_size=190, hidden_sizes=[150,100,50], batch_size=1, steps=25)\n",
    "optimizer =  torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  loss: 0.6943758453972497\n",
      "Epoch: 2  loss: 0.69370237305878\n",
      "Epoch: 3  loss: 0.6931226869822272\n",
      "Epoch: 4  loss: 0.6932854015463833\n",
      "Epoch: 5  loss: 0.6930906610844999\n",
      "Epoch: 6  loss: 0.6932126847483132\n",
      "Epoch: 7  loss: 0.6928666956365361\n",
      "Epoch: 8  loss: 0.6928274171889444\n",
      "Epoch: 9  loss: 0.6928586278746326\n",
      "Epoch: 10  loss: 0.692744545525468\n",
      "Epoch: 11  loss: 0.6926654298593582\n",
      "Epoch: 12  loss: 0.6927360055225144\n",
      "Epoch: 13  loss: 0.6927462225258532\n",
      "Epoch: 14  loss: 0.6926856611668158\n",
      "Epoch: 15  loss: 0.6926621227967934\n",
      "Epoch: 16  loss: 0.6927105606000352\n",
      "Epoch: 17  loss: 0.69268885133624\n",
      "Epoch: 18  loss: 0.692669067950269\n",
      "Epoch: 19  loss: 0.6926576346792304\n",
      "Epoch: 20  loss: 0.6927270500241588\n",
      "Epoch: 21  loss: 0.6927014498111329\n",
      "Epoch: 22  loss: 0.692469694430566\n",
      "Epoch: 23  loss: 0.6930685638295714\n",
      "Epoch: 24  loss: 0.6929333545833049\n",
      "Epoch: 25  loss: 0.6926589706266875\n",
      "Epoch: 26  loss: 0.6930278428554825\n",
      "Epoch: 27  loss: 0.6927133353102475\n",
      "Epoch: 28  loss: 0.692745605670255\n",
      "Epoch: 29  loss: 0.6928049028824514\n",
      "Epoch: 30  loss: 0.6927163549170179\n",
      "Epoch: 31  loss: 0.6924970397532312\n",
      "Epoch: 32  loss: 0.6928179199288813\n",
      "Epoch: 33  loss: 0.6927955973922808\n",
      "Epoch: 34  loss: 0.6925903577187891\n",
      "Epoch: 35  loss: 0.6927738941359969\n",
      "Epoch: 36  loss: 0.6925889848938984\n",
      "Epoch: 37  loss: 0.6925981501484901\n",
      "Epoch: 38  loss: 0.6930144492539045\n",
      "Epoch: 39  loss: 0.6927490592654285\n",
      "Epoch: 40  loss: 0.6927690322137126\n",
      "Epoch: 41  loss: 0.6927588117361503\n",
      "Epoch: 42  loss: 0.6927520242617502\n",
      "Epoch: 43  loss: 0.6926916306425603\n",
      "Epoch: 44  loss: 0.6927208621066198\n",
      "Epoch: 45  loss: 0.6926627580368236\n",
      "Epoch: 46  loss: 0.6926878172843183\n",
      "Epoch: 47  loss: 0.6927106766608099\n",
      "Epoch: 48  loss: 0.6926688560226432\n",
      "Epoch: 49  loss: 0.6927424165141459\n",
      "Epoch: 50  loss: 0.6927037164161766\n",
      "Epoch: 51  loss: 0.6926925949928756\n",
      "Epoch: 52  loss: 0.6926932138751041\n",
      "Epoch: 53  loss: 0.6927045040515675\n",
      "Epoch: 54  loss: 0.6926314968301371\n",
      "Epoch: 55  loss: 0.6926089687715101\n",
      "Epoch: 56  loss: 0.6926866547585113\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs = batch['sequence']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        inputs, labels = Variable(inputs.type(torch.FloatTensor)), Variable(labels.type(torch.LongTensor).squeeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        # clip gradients\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.75)\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "        # print statistics\n",
    "        running_loss.append(loss.data[0])\n",
    "    \n",
    "    print('Epoch: {}  loss: {}'.format(\n",
    "          epoch + 1, np.mean(np.array(running_loss))))\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        inputs = batch['sequence']\n",
    "        labels = batch['label']\n",
    "        inputs = Variable(inputs.type(torch.FloatTensor), volatile=True)\n",
    "        labels = Variable(labels.type(torch.LongTensor).squeeze(1), volatile=True)\n",
    "        \n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss_sum += loss.data[0]\n",
    "\n",
    "        predict = output.data.max(1)[1]\n",
    "        acc = predict.eq(labels.data).cpu().sum()\n",
    "        acc_sum += acc\n",
    "    return loss_sum / len(loader), acc_sum / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6886351095732822, 0.5591397849462365)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.8571\n",
      "[torch.FloatTensor of size 1]\n",
      " Variable containing:\n",
      " 0.8571\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "score = Variable(torch.randn(10,2))\n",
    "target = Variable((torch.rand(10)>0.5).long())\n",
    "lfn1 = torch.nn.CrossEntropyLoss()\n",
    "lfn2 = torch.nn.BCELoss()\n",
    "print(lfn1(score,target), lfn2(torch.nn.functional.softmax(score)[:,1],target.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.0032 -0.6803\n",
       " 1.2381  0.0608\n",
       "-0.8555  0.6880\n",
       " 0.1313 -1.2839\n",
       "-0.4654 -0.2737\n",
       "-0.7864  0.7487\n",
       " 1.4117  0.9339\n",
       " 1.9578 -0.0274\n",
       "-1.0772  2.5011\n",
       " 0.2887 -0.4335\n",
       "[torch.FloatTensor of size 10x2]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       "[torch.LongTensor of size 10]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
